{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Bright Data Direct API Access\n",
    "BRIGHTDATA_API_KEY = \"83da928bbad419ac47ff05de6107f869ed948b64d60553e3ffca02eb4bad62ca\"\n",
    "ZONE_NAME = \"web_unlocker_amazonv1\"\n",
    "\n",
    "SELLER_ID = \"A3HOUT2SFZ52HU\"\n",
    "BASE_URL = f\"https://www.amazon.com/s?i=merchant-items&me={SELLER_ID}&marketplaceID=ATVPDKIKX0DER&page={{}}\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "request_count = 0\n",
    "failed_pages = []\n",
    "all_asins = []\n",
    "\n",
    "\n",
    "def fetch_html_from_brightdata(target_url):\n",
    "    global request_count\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {BRIGHTDATA_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"zone\": ZONE_NAME,\n",
    "        \"url\": target_url,\n",
    "        \"format\": \"raw\"\n",
    "    }\n",
    "    try:\n",
    "        res = requests.post(\"https://api.brightdata.com/request\", json=data, headers=headers, timeout=30)\n",
    "        request_count += 1\n",
    "        if res.status_code == 200:\n",
    "            return res.text\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_product_details(asin):\n",
    "    product_url = f\"https://www.amazon.com/dp/{asin}\"\n",
    "    html = fetch_html_from_brightdata(product_url)\n",
    "    if not html:\n",
    "        return [\"\", asin, \"\", \"\", product_url, 0, \"0\", \"\", \"\"]\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    name = soup.select_one(\"h1 span\")\n",
    "    name = name.text.strip() if name else \"\"\n",
    "\n",
    "    rating = soup.select_one(\"span.a-icon-alt\")\n",
    "    rating = rating.text.strip() if rating else \"\"\n",
    "\n",
    "    price_whole = soup.select_one(\"span.a-price-whole\")\n",
    "    price_frac = soup.select_one(\"span.a-price-fraction\")\n",
    "    price = f\"{price_whole.text.strip()}.{price_frac.text.strip()}\" if price_whole and price_frac else \"\"\n",
    "\n",
    "    image_urls = set()\n",
    "    for img in soup.select(\"#altImages img\"):\n",
    "        url = img.get(\"src\") or img.get(\"data-src\") or img.get(\"data-image-src\")\n",
    "        if url:\n",
    "            image_urls.add(url)\n",
    "    main_image = soup.select_one(\"#landingImage\")\n",
    "    if main_image:\n",
    "        url = main_image.get(\"src\") or main_image.get(\"data-old-hires\")\n",
    "        if url:\n",
    "            image_urls.add(url)\n",
    "\n",
    "    review_text = soup.select_one(\"#acrCustomerReviewText\")\n",
    "    review_count = review_text.text.strip().split()[0].replace(\",\", \"\") if review_text else \"0\"\n",
    "\n",
    "    breadcrumbs = \" > \".join(\n",
    "        [li.text.strip() for li in soup.select(\"ul.a-unordered-list.a-horizontal.a-size-small li a\")]\n",
    "    )\n",
    "\n",
    "    bsr = \"\"\n",
    "    detail_bullets = soup.select(\"#productDetails_detailBullets_sections1 tr\")\n",
    "    for row in detail_bullets:\n",
    "        if \"Best Sellers Rank\" in row.text:\n",
    "            bsr = row.select_one(\"td\").text.strip().split(\"(\")[0].strip()\n",
    "            break\n",
    "\n",
    "    if not bsr:\n",
    "        bullets = soup.select(\"#detailBulletsWrapper_feature_div li\")\n",
    "        for li in bullets:\n",
    "            if \"Best Sellers Rank\" in li.text:\n",
    "                bsr = li.text.strip().split(\":\")[-1].split(\"(\")[0].strip()\n",
    "                break\n",
    "\n",
    "    if not bsr:\n",
    "        bestseller_link = soup.select_one('#detailBulletsWrapper_feature_div a[href*=\"bestsellers\"]')\n",
    "        if bestseller_link and bestseller_link.previous_sibling:\n",
    "            bsr_text = bestseller_link.previous_sibling.strip().split()[0]\n",
    "            bsr = bsr_text\n",
    "\n",
    "    return [name, asin, rating, price, product_url, len(image_urls), review_count, breadcrumbs, bsr]\n",
    "\n",
    "\n",
    "# Phase 1: Sequentially collect ASINs from each page\n",
    "page = 1\n",
    "while True:\n",
    "    url = BASE_URL.format(page)\n",
    "    html = fetch_html_from_brightdata(url)\n",
    "    if not html:\n",
    "        print(f\"âŒ Failed to load page {page}\")\n",
    "        failed_pages.append(page)\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    product_divs = soup.select(\"div.s-main-slot div[data-asin]:not([data-asin=''])\")\n",
    "    page_asins = [div.get(\"data-asin\", \"\") for div in product_divs if div.get(\"data-asin\")]\n",
    "\n",
    "    if not page_asins:\n",
    "        print(f\"ðŸ›‘ No products found on page {page}, stopping.\")\n",
    "        break\n",
    "\n",
    "    all_asins.extend(page_asins)\n",
    "    print(f\"ðŸ“„ Page {page} | Found {len(page_asins)} products\")\n",
    "\n",
    "    next_disabled = bool(soup.select_one(\"ul.a-pagination li.a-disabled.a-last\"))\n",
    "    if next_disabled:\n",
    "        print(\"ðŸ›‘ 'Next' button disabled. Last page reached.\")\n",
    "        break\n",
    "\n",
    "    page += 1\n",
    "\n",
    "# Phase 2: Scrape product details in parallel\n",
    "with open(\"products_with_review_counts.csv\", \"w\", newline='', encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\n",
    "        \"Name\", \"ASIN\", \"Rating\", \"Price\", \"ProductURL\",\n",
    "        \"ImageCount\", \"ReviewCount\", \"Breadcrumbs\", \"BestSellerRank\"\n",
    "    ])\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(extract_product_details, asin) for asin in all_asins]\n",
    "        for future in as_completed(futures):\n",
    "            row = future.result()\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"ðŸŽ‰ Scraping completed! File saved as 'products_with_review_counts.csv'\")\n",
    "print(f\"ðŸ“Š Total requests made via Bright Data API: {request_count}\")\n",
    "print(f\"ðŸ§¾ Total products scraped: {len(all_asins)}\")\n",
    "\n",
    "if failed_pages:\n",
    "    print(f\"âš ï¸ Pages failed after retries: {failed_pages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIGHTDATA_API_KEY = \"d0e4a478-94a7-4f42-b4f5-7b013c2a5e62\"\n",
    "ZONE_NAME = \"web_unlocker1\"\n",
    "\n",
    "input_df = pd.read_excel(\"sample_ASIN.xlsx\")\n",
    "asin_column = input_df.get(\"asin\", pd.Series())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d866ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loaded 20 valid ASINs from Excel input\n",
      "ðŸŽ‰ Scraping completed! File saved as 'products_with_review_counts.csv'\n",
      "ðŸ“Š Total requests made via Bright Data API: 20\n",
      "ðŸ§¾ Total products scraped: 20\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Bright Data Direct API Access\n",
    "BRIGHTDATA_API_KEY = \"d0e4a478-94a7-4f42-b4f5-7b013c2a5e62\"\n",
    "ZONE_NAME = \"web_unlocker1\"\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "request_count = 0\n",
    "\n",
    "\n",
    "def fetch_html_from_brightdata(target_url):\n",
    "    global request_count\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {BRIGHTDATA_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"zone\": ZONE_NAME,\n",
    "        \"url\": target_url,\n",
    "        \"format\": \"raw\"\n",
    "    }\n",
    "    try:\n",
    "        res = requests.post(\"https://api.brightdata.com/request\", json=data, headers=headers, timeout=30)\n",
    "        request_count += 1\n",
    "        if res.status_code == 200:\n",
    "            return res.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching HTML for {target_url}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_product_details(asin):\n",
    "    product_url = f\"https://www.amazon.com/dp/{asin}\"\n",
    "    html = fetch_html_from_brightdata(product_url)\n",
    "    if not html:\n",
    "        return [\"\", asin, \"No rating yet\", \"\", product_url, 0, \"No reviews yet\", \"\", \"\"]\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Product Name\n",
    "    name = soup.select_one(\"span#productTitle\")\n",
    "    name = name.text.strip() if name else \"\"\n",
    "\n",
    "    # Rating\n",
    "    rating = \"No rating yet\"\n",
    "    rating_el = soup.select_one(\"span[data-hook='rating-out-of-text']\")\n",
    "    if rating_el:\n",
    "        rating = rating_el.text.strip()\n",
    "\n",
    "    # Price\n",
    "    price = \"\"\n",
    "    price_el = soup.select_one(\"div.a-section.a-spacing-none.aok-align-center.aok-relative span.aok-offscreen\")\n",
    "    if price_el:\n",
    "        price = price_el.text.strip()\n",
    "\n",
    "    # Images\n",
    "    image_urls = set()\n",
    "    for img in soup.select(\"#altImages img\"):\n",
    "        url = img.get(\"src\") or img.get(\"data-src\") or img.get(\"data-image-src\")\n",
    "        if url:\n",
    "            image_urls.add(url)\n",
    "    main_image = soup.select_one(\"#landingImage\")\n",
    "    if main_image:\n",
    "        url = main_image.get(\"src\") or main_image.get(\"data-old-hires\")\n",
    "        if url:\n",
    "            image_urls.add(url)\n",
    "\n",
    "    # Review Count\n",
    "    review_count = \"No reviews yet\"\n",
    "    review_el = soup.select_one(\"span[data-hook='total-review-count']\")\n",
    "    if review_el:\n",
    "        review_count = review_el.text.strip().split()[0].replace(\",\", \"\")\n",
    "\n",
    "    # Breadcrumbs\n",
    "    breadcrumbs = \" > \".join([\n",
    "        a.text.strip() for a in soup.select(\"#wayfinding-breadcrumbs_feature_div ul.a-unordered-list li a\")\n",
    "    ])\n",
    "\n",
    "    # Best Seller Rank\n",
    "    bsr = \"\"\n",
    "    for section in [\"#productDetails_detailBullets_sections1 tr\", \"#detailBulletsWrapper_feature_div\"]:\n",
    "        block = soup.select(section)\n",
    "        for tag in block:\n",
    "            if \"Best Sellers Rank\" in tag.text:\n",
    "                text = tag.get_text()\n",
    "                match = re.search(r\"#([\\d,]+)\", text)\n",
    "                if match:\n",
    "                    bsr = match.group(1)\n",
    "                    break\n",
    "        if bsr:\n",
    "            break\n",
    "\n",
    "    return [name, asin, rating, price, product_url, len(image_urls), review_count, breadcrumbs, bsr]\n",
    "\n",
    "\n",
    "# Read ASINs from Excel and validate\n",
    "input_df = pd.read_excel(\"sample_ASIN.xlsx\")\n",
    "asin_column = input_df.get(\"asin\", pd.Series())\n",
    "asins_raw = asin_column.dropna().astype(str).unique().tolist()\n",
    "\n",
    "valid_asins = []\n",
    "for asin in asins_raw:\n",
    "    asin = asin.strip().upper()\n",
    "    if re.fullmatch(r\"[A-Z0-9]{10}\", asin):\n",
    "        valid_asins.append(asin)\n",
    "\n",
    "# Limit to first 20 ASINs for testing\n",
    "valid_asins = valid_asins[:20]\n",
    "\n",
    "print(f\"ðŸ“¥ Loaded {len(valid_asins)} valid ASINs from Excel input\")\n",
    "\n",
    "# Scrape products in parallel\n",
    "with open(\"products_with_review_counts.csv\", \"w\", newline='', encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\n",
    "        \"Name\", \"ASIN\", \"Rating\", \"Price\", \"ProductURL\",\n",
    "        \"ImageCount\", \"ReviewCount\", \"Breadcrumbs\", \"BestSellerRank\"\n",
    "    ])\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(extract_product_details, asin) for asin in valid_asins]\n",
    "        for future in as_completed(futures):\n",
    "            row = future.result()\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"ðŸŽ‰ Scraping completed! File saved as 'products_with_review_counts.csv'\")\n",
    "print(f\"ðŸ“Š Total requests made via Bright Data API: {request_count}\")\n",
    "print(f\"ðŸ§¾ Total products scraped: {len(valid_asins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50966e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIGHTDATA_API_KEY = \"d0e4a478-94a7-4f42-b4f5-7b013c2a5e62\"\n",
    "ZONE_NAME = \"web_unlocker1\"\n",
    "\n",
    "input_df = pd.read_excel(\"sample_ASIN.xlsx\")\n",
    "asin_column = input_df.get(\"asin\", pd.Series())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ec8b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loaded 432 valid ASINs from Excel input\n",
      "ðŸŽ‰ Scraping completed! File saved as 'products_with_review_counts.csv'\n",
      "ðŸ“Š Total requests made via Bright Data API: 432\n",
      "ðŸ§¾ Total products scraped: 432\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Bright Data Direct API Access\n",
    "BRIGHTDATA_API_KEY = \"d0e4a478-94a7-4f42-b4f5-7b013c2a5e62\"\n",
    "ZONE_NAME = \"web_unlocker1\"\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "request_count = 0\n",
    "\n",
    "def fetch_html_from_brightdata(target_url):\n",
    "    global request_count\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {BRIGHTDATA_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"zone\": ZONE_NAME,\n",
    "        \"url\": target_url,\n",
    "        \"format\": \"raw\"\n",
    "    }\n",
    "    try:\n",
    "        res = requests.post(\"https://api.brightdata.com/request\", json=data, headers=headers, timeout=30)\n",
    "        request_count += 1\n",
    "        if res.status_code == 200:\n",
    "            return res.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching HTML for {target_url}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_product_details(asin):\n",
    "    product_url = f\"https://www.amazon.com/dp/{asin}\"\n",
    "    html = fetch_html_from_brightdata(product_url)\n",
    "    if not html:\n",
    "        return [\"\", asin, \"No rating yet\", \"\", product_url, 0, \"No reviews yet\", \"\", \"\", \"Not found\"]\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Product Name\n",
    "    name = soup.select_one(\"span#productTitle\")\n",
    "    name = name.text.strip() if name else \"\"\n",
    "\n",
    "    # Rating\n",
    "    rating = \"No rating yet\"\n",
    "    rating_el = soup.select_one(\"span[data-hook='rating-out-of-text']\")\n",
    "    if rating_el:\n",
    "        rating = rating_el.text.strip()\n",
    "\n",
    "    # Price\n",
    "    price = \"\"\n",
    "    price_el = soup.select_one(\"div.a-section.a-spacing-none.aok-align-center.aok-relative span.aok-offscreen\")\n",
    "    if price_el:\n",
    "        price = price_el.text.strip()\n",
    "\n",
    "    # Images\n",
    "    image_urls = set()\n",
    "    for img in soup.select(\"#altImages img\"):\n",
    "        url = img.get(\"src\") or img.get(\"data-src\") or img.get(\"data-image-src\")\n",
    "        if url:\n",
    "            image_urls.add(url)\n",
    "    main_image = soup.select_one(\"#landingImage\")\n",
    "    if main_image:\n",
    "        url = main_image.get(\"src\") or main_image.get(\"data-old-hires\")\n",
    "        if url:\n",
    "            image_urls.add(url)\n",
    "\n",
    "    # Review Count\n",
    "    review_count = \"No reviews yet\"\n",
    "    review_el = soup.select_one(\"span[data-hook='total-review-count']\")\n",
    "    if review_el:\n",
    "        review_count = review_el.text.strip().split()[0].replace(\",\", \"\")\n",
    "\n",
    "    # Breadcrumbs\n",
    "    breadcrumbs = \" > \".join([\n",
    "        a.text.strip() for a in soup.select(\"#wayfinding-breadcrumbs_feature_div ul.a-unordered-list li a\")\n",
    "    ])\n",
    "\n",
    "    # Best Seller Rank\n",
    "    bsr = \"\"\n",
    "    for section in [\"#productDetails_detailBullets_sections1 tr\", \"#detailBulletsWrapper_feature_div\"]:\n",
    "        block = soup.select(section)\n",
    "        for tag in block:\n",
    "            if \"Best Sellers Rank\" in tag.text:\n",
    "                text = tag.get_text()\n",
    "                match = re.search(r\"#([\\d,]+)\", text)\n",
    "                if match:\n",
    "                    bsr = match.group(1)\n",
    "                    break\n",
    "        if bsr:\n",
    "            break\n",
    "\n",
    "    # Make Sure This Fits presence check\n",
    "    make_sure_fits = \"No\"\n",
    "    if soup.select_one(\"#automotive-pf-primary-view-default-make-sure-this-fits\"):\n",
    "        make_sure_fits = \"Yes\"\n",
    "\n",
    "    return [name, asin, rating, price, product_url, len(image_urls), review_count, breadcrumbs, bsr, make_sure_fits]\n",
    "\n",
    "\n",
    "# Read ASINs from Excel and validate\n",
    "input_df = pd.read_excel(\"sample_ASIN.xlsx\")\n",
    "asin_column = input_df.get(\"asin\", pd.Series())\n",
    "asins_raw = asin_column.dropna().astype(str).unique().tolist()\n",
    "\n",
    "valid_asins = []\n",
    "for asin in asins_raw:\n",
    "    asin = asin.strip().upper()\n",
    "    if re.fullmatch(r\"[A-Z0-9]{10}\", asin):\n",
    "        valid_asins.append(asin)\n",
    "\n",
    "# Limit to first 20 ASINs for testing\n",
    "#valid_asins = valid_asins[:20]\n",
    "\n",
    "print(f\"ðŸ“¥ Loaded {len(valid_asins)} valid ASINs from Excel input\")\n",
    "\n",
    "# Scrape products in parallel\n",
    "with open(\"products_with_review_counts.csv\", \"w\", newline='', encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\n",
    "        \"Name\", \"ASIN\", \"Rating\", \"Price\", \"ProductURL\",\n",
    "        \"ImageCount\", \"ReviewCount\", \"Breadcrumbs\", \"BestSellerRank\", \"MakeSureFits\"\n",
    "    ])\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(extract_product_details, asin) for asin in valid_asins]\n",
    "        for future in as_completed(futures):\n",
    "            row = future.result()\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"ðŸŽ‰ Scraping completed! File saved as 'products_with_review_counts.csv'\")\n",
    "print(f\"ðŸ“Š Total requests made via Bright Data API: {request_count}\")\n",
    "print(f\"ðŸ§¾ Total products scraped: {len(valid_asins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f7816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
